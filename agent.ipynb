{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-1.0-pro\")\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\")\n",
    "vector_store = Chroma(embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import uuid\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from typing_extensions import List, TypedDict\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA RAG model\n",
    "see: https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for ingesting data from a source and indexing it (by semantic search for our case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data with a data loader\n",
    "2. Break large documents into smaller chunks with text splitters to fit into model's finite context window\n",
    "3. Use vector store and embeddings model to store and index the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "               \"https://docs.thena.fi/thena?ref=bnbchain.ghost.io\",\n",
    "               \"https://docs.thena.fi/thena/the-onboarding\",\n",
    "               \"https://docs.thena.fi/thena/the-spot-dex/swap-guide\",\n",
    "               \"https://docs.thena.fi/thena/the-spot-dex/limit-order\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/introduction-to-fusion\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/liquidity-pools-typology\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/earn-the\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/earn-trading-fees\",\n",
    "                ),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer( \n",
    "            # Only keep post title, headers, and content from the full HTML.\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval and generation chain for taking user query at run time and retrieving data from index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a retriever to match against user query. Extending this to a tool allows models to rewrite user queries into more effective search queries. This also gives model a choice to either respond immediately or do RAG.\n",
    "2. ChatModel / LLM produces answer from prompt (which takes in user query and retrieved data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query, \n",
    "        k=2\n",
    "    )\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "Now the control flow is defined by the reasoning capabilities of LLMs. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver() # support multiple conversational turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "websearch = TavilySearchResults(max_results=2)\n",
    "\n",
    "mem_agent = create_react_agent(llm, [retrieve, websearch], checkpointer=memory)\n",
    "# display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "def create_tool_agent(llm: ChatVertexAI, tools: list, system_prompt: str):\n",
    "    \"\"\"Helper function to create agents with custom tools and system prompt\n",
    "    Args:\n",
    "        llm (ChatVertexAI): LLM for the agent\n",
    "        tools (list): list of tools the agent will use\n",
    "        system_prompt (str): text describing specific agent purpose\n",
    "\n",
    "    Returns:\n",
    "        executor (AgentExecutor): Runnable for the agent created.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Each worker node will be given a name and some tools.\n",
    "    \n",
    "    system_prompt_template = PromptTemplate(\n",
    "\n",
    "                template= system_prompt + \"\"\"\n",
    "                ONLY respond to the part of query relevant to your purpose.\n",
    "                IGNORE tasks you can't complete. \n",
    "                Use the following context to answer your query \n",
    "                if available: \\n {agent_history} \\n\n",
    "                \"\"\",\n",
    "                input_variables=[\"agent_history\"],\n",
    "            )\n",
    "\n",
    "    #define system message\n",
    "    system_message_prompt = SystemMessagePromptTemplate(prompt=system_prompt_template)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt,\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, \n",
    "                return_intermediate_steps= True, verbose = False)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt = \"\"\" You are a assistant for performing research on market sentiment of a cryto asset.\n",
    "        You are to gauge whether the sentiment is positive, negative or neutral based on news and statements\n",
    "        made by influential sources in the cryptocurrency space. Also, weigh the sentiment based on the profile of the source.\n",
    "        For example, a statement from a well-known cryptocurrency influencer may have more weight than a random twitter user with less than average number of followers.\n",
    "        You must also take into account of whether the opinion is based on facts or speculation, and if the source has a history of being accurate.\n",
    "        Use your tools to answer questions. If you do not have a tool to\n",
    "        answer the question, say so. \"\"\"\n",
    "\n",
    "sentiment_agent = create_tool_agent(llm=llm, tools = [retrieve, websearch], # todo: add twitter, governance, news tools\n",
    "              system_prompt = sentiment_prompt)\n",
    "\n",
    "risk_prompt = \"\"\" You are a assistant for performing risk analysis on a crypto asset.\n",
    "        You are to assess the risk of investing in a cryptocurrency based on the information available.\n",
    "        You must consider the technology behind the cryptocurrency, the team behind the project, the market conditions, and the regulatory environment.\n",
    "        Use your tools to complete requests. If you do not have a tool to\n",
    "       complete the request, say so. \"\"\"\n",
    "\n",
    "risk_agent = create_tool_agent(llm=llm, tools = [retrieve, websearch], \n",
    "                    system_prompt = risk_prompt)\n",
    "\n",
    "\n",
    "thena_api_prompt = \"\"\" You are an assistant for performing action such as querying user wallet status, trading, swapping and liquidity provision to THENA blockchain ecosystem based on the user's request if any.\n",
    "        Use your tools to complete requests. If you do not have a tool to\n",
    "       complete the request, say so. \"\"\"\n",
    "\n",
    "thena_api_agent = create_tool_agent(llm=llm, tools = [], \n",
    "                    system_prompt = thena_api_prompt)\n",
    "\n",
    "\n",
    "xrpl_api_prompt = \"\"\" You are an assistant for performing action such as querying user wallet status, trading, swapping and liquidity provision to Ripple Ledger (XRPL) blockchain ecosystem based on the user's request if any.\n",
    "        Use your tools to complete requests. If you do not have a tool to\n",
    "       complete the request, say so. \"\"\"\n",
    "\n",
    "xrpl_api_agent = create_tool_agent(llm=llm, tools = [], # tools for staking, LPing, swapping etc\n",
    "                    system_prompt = xrpl_api_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "system_prompt_template = PromptTemplate(\n",
    "\n",
    "      template= \"\"\" You are a helpful assistant that summarises agent history \n",
    "                      in response to the original user query below. \n",
    "                      SUMMARISE ALL THE OUTPUTS AND TOOLS USED in agent_history.\n",
    "                      The agent history is as follows: \n",
    "                        \\n{agent_history}\\n\"\"\",\n",
    "                input_variables=[\"agent_history\"],  )\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=system_prompt_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "\n",
    "comms_agent = (prompt| llm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Can you do an interactive analysis for what cryptocurrency I should buy?\"\n",
    "                )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from enum import Enum\n",
    "members = [\"Sentiment\", \"Risk\", \"THENA_API\", \"XRPL_API\", \"Mem\", \"Communicate\"]\n",
    "\n",
    "#create options map for the supervisor output parser.\n",
    "member_options = {member:member for member in members}\n",
    "\n",
    "#create Enum object\n",
    "MemberEnum = Enum('MemberEnum', member_options)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "#force Supervisor to pick from options defined above\n",
    "# return a dictionary specifying the next agent to call \n",
    "#under key next.\n",
    "class SupervisorOutput(BaseModel):\n",
    "    #defaults to communication agent\n",
    "    next: MemberEnum = MemberEnum.Communicate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "    crew of workers:  {members}. Given the following user request, \n",
    "    and crew responses respond with the worker to act next.\n",
    "    Each worker will perform a task and respond with their results and status. \n",
    "    When finished with the task, route to communicate to deliver the result to \n",
    "    user. Given the conversation and crew history below, who should act next?\n",
    "    Hint: API agents should take into account of sentiment and risk analysis.\n",
    "    Analysis should take into account mem agent history. \n",
    "    Treat mem agent like representative of user and their portfolio.\n",
    "    Select one of: {options} \n",
    "    \\n{format_instructions}\\n\"\"\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "supervisor_parser = JsonOutputParser(pydantic_object=SupervisorOutput)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_history\")\n",
    "       \n",
    "    ]\n",
    ").partial(options=str(members), members=\", \".join(members), \n",
    "    format_instructions = supervisor_parser.get_format_instructions())\n",
    "\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt | llm |supervisor_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "import operator\n",
    "\n",
    "# For agents in the crew \n",
    "def crew_nodes(state, crew_member, name):\n",
    "    #read the last message in the message history.\n",
    "    input = {'messages': [state['messages'][-1]], \n",
    "                'agent_history' : state['agent_history']}\n",
    "    result = crew_member.invoke(input)\n",
    "    #add response to the agent history.\n",
    "    return {\"agent_history\": [AIMessage(content= result[\"output\"], \n",
    "              additional_kwargs= {'intermediate_steps' : result['intermediate_steps']}, \n",
    "              name=name)]}\n",
    "\n",
    "def comms_node(state):\n",
    "    #read the last message in the message history.\n",
    "    input = {'messages': [state['messages'][-1]],\n",
    "                     'agent_history' : state['agent_history']}\n",
    "    result = comms_agent.invoke(input)\n",
    "    #respond back to the user.\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str \n",
    "    agent_history: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "mem_node = partial(crew_nodes, crew_member=mem_agent, name=\"Memory\")\n",
    "sentiment_node = partial(crew_nodes, crew_member=sentiment_agent, name=\"Sentiment\")\n",
    "risk_node = partial(crew_nodes, crew_member=risk_agent, name=\"Risk\")\n",
    "thena_api_node = partial(crew_nodes, crew_member=thena_api_agent, name=\"THENA_API\")\n",
    "xrpl_api_node = partial(crew_nodes, crew_member=xrpl_api_agent, name=\"XRPL_API\")\n",
    "\n",
    "workflow.add_node(\"Mem\", mem_node)\n",
    "workflow.add_node(\"Sentiment\", sentiment_node)\n",
    "workflow.add_node(\"Risk\", risk_node)\n",
    "workflow.add_node(\"THENA_API\", thena_api_node)\n",
    "workflow.add_node(\"XRPL_API\", xrpl_api_node)\n",
    "workflow.add_node(\"Communicate\", comms_node )\n",
    "workflow.add_node(\"Supervisor\", supervisor_chain)\n",
    "workflow.set_entry_point(\"Supervisor\")\n",
    "workflow.add_edge('Mem', \"Supervisor\")\n",
    "workflow.add_edge('Sentiment', \"Supervisor\")\n",
    "workflow.add_edge('Risk', \"Supervisor\")\n",
    "workflow.add_edge('THENA_API', \"Supervisor\")\n",
    "workflow.add_edge('XRPL_API', \"Supervisor\")\n",
    "workflow.add_edge('Communicate', END) \n",
    "# end loop at communication agent.\n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "workflow.add_conditional_edges(\"Supervisor\", lambda x: x[\"next\"], member_options)\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests / Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can you perform an interactive analysis for what cryptocurrency I should buy?\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
