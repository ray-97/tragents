{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-1.0-pro\")\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\")\n",
    "vector_store = Chroma(embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import uuid\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from typing_extensions import List, TypedDict\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA RAG model\n",
    "see: https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for ingesting data from a source and indexing it (by semantic search for our case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data with a data loader\n",
    "2. Break large documents into smaller chunks with text splitters to fit into model's finite context window\n",
    "3. Use vector store and embeddings model to store and index the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\", # general llm agent blog for the curious\n",
    "               # help llm make sense of functions\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/snippets.html\",\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/xrpl.models.html\",\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/xrpl.wallet.html\",\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/xrpl.clients.html\",\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/xrpl.transaction.html\",\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/xrpl.ledger.html\",\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/xrpl.core.addresscodec.html\",\n",
    "                \"https://xrpl-py.readthedocs.io/en/stable/source/xrpl.utils.html\",\n",
    "                # context for answering questions related to the XRPL\n",
    "                \"https://xrpl.org/docs/introduction/what-is-the-xrp-ledger\",\n",
    "                \"https://xrpl.org/docs/introduction/what-is-xrp\",\n",
    "                \"https://xrpl.org/docs/introduction/crypto-wallets\",\n",
    "                \"https://xrpl.org/docs/introduction/transactions-and-requests\",\n",
    "                # context for answering questions related to the THENA protocol\n",
    "               \"https://docs.thena.fi/thena?ref=bnbchain.ghost.io\",\n",
    "               \"https://docs.thena.fi/thena/the-onboarding\",\n",
    "               \"https://docs.thena.fi/thena/the-spot-dex/swap-guide\",\n",
    "               \"https://docs.thena.fi/thena/the-spot-dex/limit-order\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/introduction-to-fusion\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/liquidity-pools-typology\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/earn-the\",\n",
    "               \"https://docs.thena.fi/thena/the-liquidity-pools/earn-trading-fees\",\n",
    "                ),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer( \n",
    "            # Only keep post title, headers, and content from the full HTML.\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval and generation chain for taking user query at run time and retrieving data from index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a retriever to match against user query. Extending this to a tool allows models to rewrite user queries into more effective search queries. This also gives model a choice to either respond immediately or do RAG.\n",
    "2. ChatModel / LLM produces answer from prompt (which takes in user query and retrieved data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query, \n",
    "        k=2\n",
    "    )\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "Now the control flow is defined by the reasoning capabilities of LLMs. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver() # support multiple conversational turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "websearch = TavilySearchResults(max_results=2)\n",
    "\n",
    "mem_agent = create_react_agent(llm, [retrieve, websearch], checkpointer=memory)\n",
    "# display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "def create_tool_agent(llm: ChatVertexAI, tools: list, system_prompt: str):\n",
    "    \"\"\"Helper function to create agents with custom tools and system prompt\n",
    "    Args:\n",
    "        llm (ChatVertexAI): LLM for the agent\n",
    "        tools (list): list of tools the agent will use\n",
    "        system_prompt (str): text describing specific agent purpose\n",
    "\n",
    "    Returns:\n",
    "        executor (AgentExecutor): Runnable for the agent created.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Each worker node will be given a name and some tools.\n",
    "    \n",
    "    system_prompt_template = PromptTemplate(\n",
    "\n",
    "                template= system_prompt + \"\"\"\n",
    "                ONLY respond to the part of query relevant to your purpose.\n",
    "                IGNORE tasks you can't complete. \n",
    "                Use the following context to answer your query \n",
    "                if available: \\n {agent_history} \\n\n",
    "                \"\"\",\n",
    "                input_variables=[\"agent_history\"],\n",
    "            )\n",
    "\n",
    "    #define system message\n",
    "    system_message_prompt = SystemMessagePromptTemplate(prompt=system_prompt_template)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt,\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, \n",
    "                return_intermediate_steps= True, verbose = False)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XRPL DEX Setup for agent tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xrpl.clients import JsonRpcClient\n",
    "from xrpl.clients import WebsocketClient\n",
    "from xrpl.wallet import generate_faucet_wallet\n",
    "from xrpl.models.transactions import Payment\n",
    "from xrpl.utils import xrp_to_drops\n",
    "from xrpl.transaction import submit_and_wait\n",
    "from xrpl.models.requests.account_info import AccountInfo\n",
    "from xrpl.models.transactions import OfferCreate, OfferCancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_rpc_client():\n",
    "    \"\"\"Create a JSON-RPC client for the XRP Ledger.\"\"\"\n",
    "    return JsonRpcClient(\"https://s.altnet.rippletest.net:51234/\")\n",
    "\n",
    "def get_websocket_client():\n",
    "    \"\"\"Create a WebSocket client for the XRP Ledger.\"\"\"\n",
    "    return WebsocketClient(\"wss://s.altnet.rippletest.net:51233\")\n",
    "\n",
    "def generate_wallet(client):\n",
    "    \"\"\"Generate a new wallet using the XRP Ledger.\"\"\"\n",
    "    return generate_faucet_wallet(client, debug=False)\n",
    "\n",
    "def sign_and_submit_payment_txn(client, wallet, destination, amount):\n",
    "    \"\"\"Sign and submit a payment transaction to the XRP Ledger.\"\"\"\n",
    "    payment = Payment(\n",
    "        account=wallet.classic_address,\n",
    "        amount=xrp_to_drops(amount),\n",
    "        destination=destination\n",
    "    )\n",
    "    response = submit_and_wait(payment, client, wallet)\n",
    "    return response\n",
    "\n",
    "def get_account_info(client, address):\n",
    "    \"\"\"Get account information from the XRP Ledger.\"\"\"\n",
    "    info = AccountInfo(\n",
    "        account=address,\n",
    "        edger_index=\"validated\",\n",
    "        strict=True,\n",
    "    )\n",
    "    response = client.request(info)\n",
    "    return response\n",
    "    \n",
    "### Trading specific functions\n",
    "# https://xrpl.org/docs/tutorials/how-tos/use-tokens/trade-in-the-decentralized-exchange#interactive-lookupoffers\n",
    "# https://xrpl-py.readthedocs.io/en/stable/source/xrpl.models.transactions.html -> look for OfferCreate and OfferCancel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pprint\n",
    "from decimal import Decimal\n",
    "\n",
    "from xrpl.asyncio.clients import AsyncWebsocketClient\n",
    "from xrpl.asyncio.transaction import (\n",
    "    autofill_and_sign,\n",
    "    submit_and_wait,\n",
    ")\n",
    "from xrpl.asyncio.wallet import generate_faucet_wallet\n",
    "from xrpl.models.currencies import (\n",
    "    IssuedCurrency,\n",
    "    XRP,\n",
    ")\n",
    "from xrpl.models.requests import (\n",
    "    AccountLines,\n",
    "    AccountOffers,\n",
    "    BookOffers,\n",
    ")\n",
    "from xrpl.models.transactions import OfferCreate\n",
    "from xrpl.utils import (\n",
    "    drops_to_xrp,\n",
    "    get_balance_changes,\n",
    "    xrp_to_drops,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials from the Testnet Faucet -----------------------------------\n",
    "# note: https://xrpl.org/docs/concepts/transactions/secure-signing is the recommended way to sign.\n",
    "# Testnet info can possibly be reset at any time.\n",
    "# Address: rGNh4AB95PZ1XrE8BSGDpGWE4n5cFsX1ha\n",
    "# Secret: sEd7mPHej64humD6kPwvxJYGpFX8axn\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebsocketClient(\"wss://s.altnet.rippletest.net:51233\") as client:\n",
    "\n",
    "      print(\"Requesting addresses from the Testnet faucet...\")\n",
    "      wallet = await generate_faucet_wallet(client, debug=True)\n",
    "\n",
    "\n",
    "    # after exiting the context, the client is closed\n",
    "\n",
    "asyncio.run(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proposed trade. ------------------------------------------------\n",
    "      # Technically you don't need to specify the amounts (in the \"value\" field)\n",
    "      # to look up order books using book_offers, but for this tutorial we reuse\n",
    "      # these variables to construct the actual Offer later.\n",
    "      #\n",
    "      # Note that XRP is represented as drops, whereas any other currency is\n",
    "      # represented as a decimal value.\n",
    "      we_want = {\n",
    "          \"currency\": IssuedCurrency(\n",
    "              currency=\"TST\",\n",
    "              issuer=\"rP9jPyP5kyvFRb6ZiRghAGw5u8SGAmU4bd\"\n",
    "          ),\n",
    "          \"value\": \"25\",\n",
    "      }\n",
    "\n",
    "      we_spend = {\n",
    "          \"currency\": XRP(),\n",
    "          # 25 TST * 10 XRP per TST * 15% financial exchange (FX) cost\n",
    "          \"value\": xrp_to_drops(25 * 10 * 1.15),\n",
    "      }\n",
    "\n",
    "      # \"Quality\" is defined as TakerPays / TakerGets. The lower the \"quality\"\n",
    "      # number, the better the proposed exchange rate is for the taker.\n",
    "      # The quality is rounded to a number of significant digits based on the\n",
    "      # issuer's TickSize value (or the lesser of the two for token-token trades).\n",
    "      proposed_quality = Decimal(we_spend[\"value\"]) / Decimal(we_want[\"value\"])\n",
    "\n",
    "      # Look up Offers. -----------------------------------------------------------\n",
    "      # To buy TST, look up Offers where \"TakerGets\" is TST:\n",
    "      print(\"Requesting orderbook information...\")\n",
    "      orderbook_info = await client.request(\n",
    "          BookOffers(\n",
    "              taker=wallet.address,\n",
    "              ledger_index=\"current\",\n",
    "              taker_gets=we_want[\"currency\"],\n",
    "              taker_pays=we_spend[\"currency\"],\n",
    "              limit=10,\n",
    "          )\n",
    "      )\n",
    "      print(f\"Orderbook:\\n{pprint.pformat(orderbook_info.result)}\")\n",
    "\n",
    "      # Estimate whether a proposed Offer would execute immediately, and...\n",
    "      # If so, how much of it? (Partial execution is possible)\n",
    "      # If not, how much liquidity is above it? (How deep in the order book would\n",
    "      # other Offers have to go before ours would get taken?)\n",
    "      # Note: These estimates can be thrown off by rounding if the token issuer\n",
    "      # uses a TickSize setting other than the default (15). In that case, you\n",
    "      # can increase the TakerGets amount of your final Offer to compensate.\n",
    "\n",
    "      offers = orderbook_info.result.get(\"offers\", [])\n",
    "      want_amt = Decimal(we_want[\"value\"])\n",
    "      running_total = Decimal(0)\n",
    "      if len(offers) == 0:\n",
    "          print(\"No Offers in the matching book. Offer probably won't execute immediately.\")\n",
    "      else:\n",
    "          for o in offers:\n",
    "              if Decimal(o[\"quality\"]) <= proposed_quality:\n",
    "                  print(f\"Matching Offer found, funded with {o.get('owner_funds')} \"\n",
    "                        f\"{we_want['currency']}\")\n",
    "                  running_total += Decimal(o.get(\"owner_funds\", Decimal(0)))\n",
    "                  if running_total >= want_amt:\n",
    "                      print(\"Full Offer will probably fill\")\n",
    "                      break\n",
    "              else:\n",
    "                  # Offers are in ascending quality order, so no others after this\n",
    "                  # will match either\n",
    "                  print(\"Remaining orders too expensive.\")\n",
    "                  break\n",
    "\n",
    "          print(f\"Total matched: {min(running_total, want_amt)} {we_want['currency']}\")\n",
    "          if 0 < running_total < want_amt:\n",
    "              print(f\"Remaining {want_amt - running_total} {we_want['currency']} \"\n",
    "                    \"would probably be placed on top of the order book.\")\n",
    "\n",
    "      if running_total == 0:\n",
    "          # If part of the Offer was expected to cross, then the rest would be placed\n",
    "          # at the top of the order book. If none did, then there might be other\n",
    "          # Offers going the same direction as ours already on the books with an\n",
    "          # equal or better rate. This code counts how much liquidity is likely to be\n",
    "          # above ours.\n",
    "          #\n",
    "          # Unlike above, this time we check for Offers going the same direction as\n",
    "          # ours, so TakerGets and TakerPays are reversed from the previous\n",
    "          # book_offers request.\n",
    "\n",
    "          print(\"Requesting second orderbook information...\")\n",
    "          orderbook2_info = await client.request(\n",
    "              BookOffers(\n",
    "                  taker=wallet.address,\n",
    "                  ledger_index=\"current\",\n",
    "                  taker_gets=we_spend[\"currency\"],\n",
    "                  taker_pays=we_want[\"currency\"],\n",
    "                  limit=10,\n",
    "              )\n",
    "          )\n",
    "          print(f\"Orderbook2:\\n{pprint.pformat(orderbook2_info.result)}\")\n",
    "\n",
    "          # Since TakerGets/TakerPays are reversed, the quality is the inverse.\n",
    "          # You could also calculate this as 1 / proposed_quality.\n",
    "          offered_quality = Decimal(we_want[\"value\"]) / Decimal(we_spend[\"value\"])\n",
    "\n",
    "          tally_currency = we_spend[\"currency\"]\n",
    "          if isinstance(tally_currency, XRP):\n",
    "              tally_currency = f\"drops of {tally_currency}\"\n",
    "\n",
    "          offers2 = orderbook2_info.result.get(\"offers\", [])\n",
    "          running_total2 = Decimal(0)\n",
    "          if len(offers2) == 0:\n",
    "              print(\"No similar Offers in the book. Ours would be the first.\")\n",
    "          else:\n",
    "              for o in offers2:\n",
    "                  if Decimal(o[\"quality\"]) <= offered_quality:\n",
    "                      print(f\"Existing offer found, funded with {o.get('owner_funds')} \"\n",
    "                            f\"{tally_currency}\")\n",
    "                      running_total2 += Decimal(o.get(\"owner_funds\", Decimal(0)))\n",
    "                  else:\n",
    "                      print(\"Remaining orders are below where ours would be placed.\")\n",
    "                      break\n",
    "\n",
    "              print(f\"Our Offer would be placed below at least {running_total2} \"\n",
    "                    f\"{tally_currency}\")\n",
    "              if 0 < running_total2 < want_amt:\n",
    "                  print(f\"Remaining {want_amt - running_total2} {tally_currency} \"\n",
    "                        \"will probably be placed on top of the order book.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send OfferCreate transaction ----------------------------------------------\n",
    "\n",
    "        # For this tutorial, we already know that TST is pegged to\n",
    "        # XRP at a rate of approximately 10:1 plus spread, so we use\n",
    "        # hard-coded TakerGets and TakerPays amounts.\n",
    "\n",
    "        tx = OfferCreate(\n",
    "            account=wallet.address,\n",
    "            taker_gets=we_spend[\"value\"],\n",
    "            taker_pays=we_want[\"currency\"].to_amount(we_want[\"value\"]),\n",
    "        )\n",
    "\n",
    "        # Sign and autofill the transaction (ready to submit)\n",
    "        signed_tx = await autofill_and_sign(tx, client, wallet)\n",
    "        print(\"Transaction:\", signed_tx)\n",
    "\n",
    "        # Submit the transaction and wait for response (validated or rejected)\n",
    "        print(\"Sending OfferCreate transaction...\")\n",
    "        result = await submit_and_wait(signed_tx, client)\n",
    "        if result.is_successful():\n",
    "            print(f\"Transaction succeeded: \"\n",
    "                  f\"https://testnet.xrpl.org/transactions/{signed_tx.get_hash()}\")\n",
    "        else:\n",
    "            raise Exception(f\"Error sending transaction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # wait for validation 4-7 seconds\n",
    "\n",
    "# Check metadata of validated transaction's metadata, instead of tentative txn metadata which can be different from final result ------------------------------------------------------------\n",
    "\n",
    "# In case of an OfferCreate transaction, likely results include:\n",
    "\n",
    "# Some or all of the Offer may have been filled by matching with existing Offers in the ledger.\n",
    "# The unmatched remainder, if any, has been placed into the ledger to await new matching Offers.\n",
    "# Other bookkeeping may have occurred, such as removing expired or unfunded Offers that would have matched.\n",
    "        \n",
    "        balance_changes = get_balance_changes(result.result[\"meta\"])\n",
    "        print(f\"Balance Changes:\\n{pprint.pformat(balance_changes)}\")\n",
    "\n",
    "        # For educational purposes the transaction metadata is analyzed manually in the\n",
    "        # following section. However, there is also a get_order_book_changes(metadata)\n",
    "        # utility function available in the xrpl library, which is generally the easier\n",
    "        # and preferred choice for parsing the metadata and computing orderbook changes.\n",
    "\n",
    "        # Helper to convert an XRPL amount to a string for display\n",
    "        def amt_str(amt) -> str:\n",
    "            if isinstance(amt, str):\n",
    "                return f\"{drops_to_xrp(amt)} XRP\"\n",
    "            else:\n",
    "                return f\"{amt['value']} {amt['currency']}.{amt['issuer']}\"\n",
    "\n",
    "        offers_affected = 0\n",
    "        for affnode in result.result[\"meta\"][\"AffectedNodes\"]:\n",
    "            if \"ModifiedNode\" in affnode:\n",
    "                if affnode[\"ModifiedNode\"][\"LedgerEntryType\"] == \"Offer\":\n",
    "                    # Usually a ModifiedNode of type Offer indicates a previous Offer that\n",
    "                    # was partially consumed by this one.\n",
    "                    offers_affected += 1\n",
    "            elif \"DeletedNode\" in affnode:\n",
    "                if affnode[\"DeletedNode\"][\"LedgerEntryType\"] == \"Offer\":\n",
    "                    # The removed Offer may have been fully consumed, or it may have been\n",
    "                    # found to be expired or unfunded.\n",
    "                    offers_affected += 1\n",
    "            elif \"CreatedNode\" in affnode:\n",
    "                if affnode[\"CreatedNode\"][\"LedgerEntryType\"] == \"RippleState\":\n",
    "                    print(\"Created a trust line.\")\n",
    "                elif affnode[\"CreatedNode\"][\"LedgerEntryType\"] == \"Offer\":\n",
    "                    offer = affnode[\"CreatedNode\"][\"NewFields\"]\n",
    "                    print(f\"Created an Offer owned by {offer['Account']} with \"\n",
    "                          f\"TakerGets={amt_str(offer['TakerGets'])} and \"\n",
    "                          f\"TakerPays={amt_str(offer['TakerPays'])}.\")\n",
    "\n",
    "        print(f\"Modified or removed {offers_affected} matching Offer(s)\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Check balances ------------------------------------------------------------\n",
    "        print(\"Getting address balances as of validated ledger...\")\n",
    "        balances = await client.request(\n",
    "            AccountLines(\n",
    "                account=wallet.address,\n",
    "                ledger_index=\"validated\",\n",
    "            )\n",
    "        )\n",
    "        pprint.pp(balances.result)\n",
    "\n",
    "        # Check Offers --------------------------------------------------------------\n",
    "        print(f\"Getting outstanding Offers from {wallet.address} \"\n",
    "              f\"as of validated ledger...\")\n",
    "        acct_offers = await client.request(\n",
    "            AccountOffers(\n",
    "                account=wallet.address,\n",
    "                ledger_index=\"validated\",\n",
    "            )\n",
    "        )\n",
    "        pprint.pp(acct_offers.result)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuation of agent prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt = \"\"\" You are a assistant for performing research on market sentiment of a cryto asset.\n",
    "        You are to gauge whether the sentiment is positive, negative or neutral based on news and statements\n",
    "        made by influential sources in the cryptocurrency space. Also, weigh the sentiment based on the profile of the source.\n",
    "        For example, a statement from a well-known cryptocurrency influencer may have more weight than a random twitter user with less than average number of followers.\n",
    "        You must also take into account of whether the opinion is based on facts or speculation, and if the source has a history of being accurate.\n",
    "        Use your tools to answer questions. If you do not have a tool to\n",
    "        answer the question, say so. \"\"\"\n",
    "\n",
    "sentiment_agent = create_tool_agent(llm=llm, tools = [retrieve, websearch], # todo: add twitter, governance, news tools\n",
    "              system_prompt = sentiment_prompt)\n",
    "\n",
    "risk_prompt = \"\"\" You are a assistant for performing risk analysis on a crypto asset.\n",
    "        You are to assess the risk of investing in a cryptocurrency based on the information available.\n",
    "        You must consider the technology behind the cryptocurrency, the team behind the project, the market conditions, and the regulatory environment.\n",
    "        Use your tools to complete requests. If you do not have a tool to\n",
    "       complete the request, say so. \"\"\"\n",
    "\n",
    "risk_agent = create_tool_agent(llm=llm, tools = [retrieve, websearch], \n",
    "                    system_prompt = risk_prompt)\n",
    "\n",
    "\n",
    "thena_api_prompt = \"\"\" You are an assistant for performing action such as querying user wallet status, trading, swapping and liquidity provision to THENA blockchain ecosystem based on the user's request if any.\n",
    "        Use your tools to complete requests. If you do not have a tool to\n",
    "       complete the request, say so. \"\"\"\n",
    "\n",
    "thena_api_agent = create_tool_agent(llm=llm, tools = [], \n",
    "                    system_prompt = thena_api_prompt)\n",
    "\n",
    "\n",
    "xrpl_api_prompt = \"\"\" You are an assistant for performing action such as querying user wallet status, trading, swapping and liquidity provision to Ripple Ledger (XRPL) blockchain ecosystem based on the user's request if any.\n",
    "        Use your tools to complete requests. If you do not have a tool to\n",
    "       complete the request, say so. \"\"\"\n",
    "\n",
    "xrpl_api_agent = create_tool_agent(llm=llm, tools = [], # tools for staking, LPing, swapping etc\n",
    "                    system_prompt = xrpl_api_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "system_prompt_template = PromptTemplate(\n",
    "\n",
    "      template= \"\"\" You are a helpful assistant that summarises agent history \n",
    "                      in response to the original user query below. \n",
    "                      SUMMARISE ALL THE OUTPUTS AND TOOLS USED in agent_history.\n",
    "                      The agent history is as follows: \n",
    "                        \\n{agent_history}\\n\"\"\",\n",
    "                input_variables=[\"agent_history\"],  )\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=system_prompt_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "\n",
    "comms_agent = (prompt| llm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Can you do an interactive analysis for what cryptocurrency I should buy?\"\n",
    "                )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from enum import Enum\n",
    "members = [\"Sentiment\", \"Risk\", \"THENA_API\", \"XRPL_API\", \"Mem\", \"Communicate\"]\n",
    "\n",
    "#create options map for the supervisor output parser.\n",
    "member_options = {member:member for member in members}\n",
    "\n",
    "#create Enum object\n",
    "MemberEnum = Enum('MemberEnum', member_options)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "#force Supervisor to pick from options defined above\n",
    "# return a dictionary specifying the next agent to call \n",
    "#under key next.\n",
    "class SupervisorOutput(BaseModel):\n",
    "    #defaults to communication agent\n",
    "    next: MemberEnum = MemberEnum.Communicate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "    crew of workers:  {members}. Given the following user request, \n",
    "    and crew responses respond with the worker to act next.\n",
    "    Each worker will perform a task and respond with their results and status. \n",
    "    When finished with the task, route to communicate to deliver the result to \n",
    "    user. Given the conversation and crew history below, who should act next?\n",
    "    Hint: API agents should take into account of sentiment and risk analysis.\n",
    "    Analysis should take into account mem agent history. \n",
    "    Treat mem agent like representative of user and their portfolio.\n",
    "    Select one of: {options} \n",
    "    \\n{format_instructions}\\n\"\"\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "supervisor_parser = JsonOutputParser(pydantic_object=SupervisorOutput)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_history\")\n",
    "       \n",
    "    ]\n",
    ").partial(options=str(members), members=\", \".join(members), \n",
    "    format_instructions = supervisor_parser.get_format_instructions())\n",
    "\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt | llm |supervisor_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "import operator\n",
    "\n",
    "# For agents in the crew \n",
    "def crew_nodes(state, crew_member, name):\n",
    "    #read the last message in the message history.\n",
    "    input = {'messages': [state['messages'][-1]], \n",
    "                'agent_history' : state['agent_history']}\n",
    "    result = crew_member.invoke(input)\n",
    "    #add response to the agent history.\n",
    "    return {\"agent_history\": [AIMessage(content= result[\"output\"], \n",
    "              additional_kwargs= {'intermediate_steps' : result['intermediate_steps']}, \n",
    "              name=name)]}\n",
    "\n",
    "def comms_node(state):\n",
    "    #read the last message in the message history.\n",
    "    input = {'messages': [state['messages'][-1]],\n",
    "                     'agent_history' : state['agent_history']}\n",
    "    result = comms_agent.invoke(input)\n",
    "    #respond back to the user.\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str \n",
    "    agent_history: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "mem_node = partial(crew_nodes, crew_member=mem_agent, name=\"Memory\")\n",
    "sentiment_node = partial(crew_nodes, crew_member=sentiment_agent, name=\"Sentiment\")\n",
    "risk_node = partial(crew_nodes, crew_member=risk_agent, name=\"Risk\")\n",
    "thena_api_node = partial(crew_nodes, crew_member=thena_api_agent, name=\"THENA_API\")\n",
    "xrpl_api_node = partial(crew_nodes, crew_member=xrpl_api_agent, name=\"XRPL_API\")\n",
    "\n",
    "workflow.add_node(\"Mem\", mem_node)\n",
    "workflow.add_node(\"Sentiment\", sentiment_node)\n",
    "workflow.add_node(\"Risk\", risk_node)\n",
    "workflow.add_node(\"THENA_API\", thena_api_node)\n",
    "workflow.add_node(\"XRPL_API\", xrpl_api_node)\n",
    "workflow.add_node(\"Communicate\", comms_node )\n",
    "workflow.add_node(\"Supervisor\", supervisor_chain)\n",
    "workflow.set_entry_point(\"Supervisor\")\n",
    "workflow.add_edge('Mem', \"Supervisor\")\n",
    "workflow.add_edge('Sentiment', \"Supervisor\")\n",
    "workflow.add_edge('Risk', \"Supervisor\")\n",
    "workflow.add_edge('THENA_API', \"Supervisor\")\n",
    "workflow.add_edge('XRPL_API', \"Supervisor\")\n",
    "workflow.add_edge('Communicate', END) \n",
    "# end loop at communication agent.\n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "workflow.add_conditional_edges(\"Supervisor\", lambda x: x[\"next\"], member_options)\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests / Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can you perform an interactive analysis for what cryptocurrency I should buy?\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
